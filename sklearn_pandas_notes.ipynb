{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on SKlearn and Pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied over from `december_logistic_regression_people_analytics.ipynb` on Jan 16 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddba8697-bfa3-4955-9f0c-5d5dd107739e",
   "metadata": {},
   "source": [
    "# SKLearn notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0945f4f2-618a-4321-8efb-402e8b8db141",
   "metadata": {},
   "source": [
    "The  sklearn preprocessing methods have fit and transform methods. `fit` fits the data, while `transform` returns you back data that's centered or one hot encoded. `fit_transform` can do both.\n",
    "\n",
    "The general sklearn pipeline is as follows:\n",
    "1. The `sklearn.model_selection` module will help you split data / randomly sample your data.\n",
    "    1. Use `train_test_split` to split data into train and test\n",
    "    1. The `StratifiedKFold` class is a \"splitter class / cross validator\" that you can use to do cross validation. \n",
    "    1. The `GridSearchCV` is a hyper-paramater optimizer, that will do exhaustive search over specified parameter values for an estimator. You can also call `cross_validate` if you don't need to do full hyperparameter optimization, and you just want average model performance across your K-fold cross validator.\n",
    "1. The `sklearn.preprocessing` module will help you scale / transform columns so that theyre ready for a model. \n",
    "    1. Use a `ColumnTransformer` to do `StandardScaler` or `OneHotEncoder` / `LabelEncoder`. Use the `fit` and `transform` of these preprocessors to make life easier. `fit` fits the scaler/encoder to your data, and `transform` transforms your data with the appropriate rule. `fit_transform` does both at once. You may also need an imputer, such as a `SimpleImputer`.\n",
    "    1. A `ColumnTransformer` will only work with named columns in a Pandas DataFrame, which means you can only use it at the first step of a pipeline (otherwise upstream steps make things into numpy arrays which arent named -- where you can use the numeric indexer if really needed). A `sklearn.compose.make_column_selector` can help with unnamed columns as well. Creating `passthrough` columnn selectors are pretty useful as well.\n",
    "    1. A `FunctionTransformer` can help you construct a transformer from an arbitrary callable. This won't store data between train and test set (eg. if you wanted to impute data with medians).\n",
    "1. Use the `Pipeline` method to make a pipeline with your feature engineering and classifier, `LogisticRegression` in this case.\n",
    "    1. A `sklearn.pipeline.FeatureUnion` can concatenate results of multiple transformer objects. Eg. you do PCA and SVD on the same input dataset. This can be useful for performing different operations for different types of columns. This [blog](https://adamnovotny.com/blog/custom-scikit-learn-pipeline.html) has some good examples.\n",
    "1. Call the `fit` method of your pipeline to fit the data. At this point you can do more advanced model selection:\n",
    "    1. You can do a `GridSearchCV` over a `StratifiedKFold` to do hyperparameter optimization\n",
    "    1. You can also `model_selection.cross_validate` over a `StratifiedKFold` to get the average model performance, without doing hyperparameter optimization. In case you want to do anything more advanced using the `StratifiedKFold`, you will likely need to get the train and test indices from the skf. You can do this by looping through `for train_index, test_index in skf.split(X, y)`.\n",
    "1. The `sklearn.metrics` module will get you precision recall and ROC curves. \n",
    "    1. `metrics.precision_recall_curve` and `metrics.roc_curve` will take predicted probabilities and true outcomes to generate PR and ROC curves with many thresholds. \n",
    "    1. `metrics.accuracy_score` and `metrics.f1_score` will take predicted and true outcomes and give you an accuracy or f1 score. Be careful about whether the function takes `y_pred` (0 or 1) or predicted probabilities as an input.\n",
    "1. To get the predictions of a classifier, you need to use the `predict_proba` and `decision_function` methods (depending on the classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1615ac6",
   "metadata": {},
   "source": [
    "SKlearn tutorial video notes [link](https://www.youtube.com/watch?v=0Lt9w-BxKFQ&t=217s). \n",
    "\n",
    "1. Wine quality dataset, predict quality based on things like acidity, density, alcohol content, pH etc.\n",
    "1. How to process nulls. `wine.isnull().sum`\n",
    "1. Make a LabelEncoder fit/transform on labels.\n",
    "1. Define X and y. You can drop a single column as follows: `wine.drop('quality', axis = 1)`\n",
    "1. `train_test_split`\n",
    "1. Scale input columns with a `StandardScaler`, `X_train = sc.fit_transform(X_train)` (maybe using a pipeline is better).\n",
    "1. Random forest initiate: `rfc = RandomForestClassifier(n_estimators=200)`. Works well in medium sized data\n",
    "1. Fit the random forest: `rfc.fit(X_train, y_train)`\n",
    "1. Predict: `pred_rfc = rfc.predict(X_test)`, might need to use `predict_proba` sometimes\n",
    "1. See how well the model performed: `print(classification_report(y_test, pred_rfc))`\n",
    "1. SVM initiate `clf = svm.SVC()`. Then fit / predict based on them. SVM does better with small datasets generally.\n",
    "1. Neural network (does well with huge data, images, text): `mlpc = MLPClassifier(hidden_layer_sizes = (11, 11, 11), max_iter = 500)`\n",
    "1. Accuracy score: `cm = metrics.accuracy_score(y_test, pred_rfc)` \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to ask you some questions about topics in applied machine learning that doen't really come up in textbooks. I'd love to get your thoughts on these, as well as any other important topics you see in practice that don't come up in most classes.\n",
    "\n",
    "How to impute `NA` values. \n",
    "1. Right now I see a couple possibilities here. One approach is to use `sklearn`'s `SimpleImputer` or `IterativeImputer` to do the work for you. The other is to some exploratory data analysis to look at major patterns in the data (eg. Titanic missing age and survival are very correlated with passenger class and gender, so imputing age with class/gender medians might make sense). I'd love to know what strategies you use to impute missing data. \n",
    "1. I also have noticed that how you impute `NA`'s has a big impact on model performance. How do you keep track of what the best imputation strategy is? Do you treat it as a hyperparameter to be optimized with other variables (eg. tree depth), or do you just deal with it at the training stage (eg. do a bit of k-fold cross validation to understand the best imputation strategy, then fix the imputation strategy and move on?)\n",
    "\n",
    "Model validation / k-fold cross validation.\n",
    "1. How to choose the value of k. Eg the Titanic dataset has ~800 observations. My current workflow when testing whether doing something (eg. bucketing features) helps the model or not is doing 10-fold cross validation over a `StratifiedKFold` in sklearn and plot the ROC curves for each test fold. I'll average the ROC curves to get an AUC metric that I use as the overall judge of model performance. Does this seem reasonable? Would you add other metrics? Eg. I also look at AUPRC and test set accuracy in the test folds.\n",
    "\n",
    "How to deal with imbalanced classes\n",
    "\n",
    "Minimizing leakage between train and test sets\n",
    "\n",
    "How to keep track of all the different hyperparameters (eg. how to impute, tree depth etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `pd.merge` to merge datasets\n",
    "1. `df.loc` vs `df.iloc`: integer indexing\n",
    "1. `value_counts` also `data.groupby(groups).size().reset_index().rename(columns = {0:'size'})`\n",
    "1. `df.isna().sum()`\n",
    "1. `df['column'].astype('int')`\n",
    "1. `A.query('Age == median')` to get rows where Age is equal to median\n",
    "1. Dummy column `df.assign(is_ib_test = (df.channel == 'instant_book').astype('int'))`\n",
    "1. Make new column as a function of the value in another column: `df_n_reviews_race.loc[df_n_reviews_race.n_reviews == '5', 'n_reviews'] = '5+'`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46f85706",
   "metadata": {},
   "source": [
    "Pandas tips from sklearn video\n",
    "\n",
    "1. `data.isnull.sum()`\n",
    "1. `pd.cut(wine['quality'], bins = bins, labels = group_names)`\n",
    "1. `wine['quality'].value_counts()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78445a2c",
   "metadata": {},
   "source": [
    "Matt Harrison Effective Pandas video notes [link](https://www.youtube.com/watch?v=UURvPeczxJI&t=505s).\n",
    "1. Pandas is built on top of NumPy, a package to make numerical computation faster in Python.\n",
    "1. `autos[cols].dtypes` you see a lot of `int64` / `float64` (fast) or `object` (mixed type or string).\n",
    "1. `autos[cols].memory_usage(deep = True)` \n",
    "1. You can save memory by converting 16 byte integers to 8 byte integers etc. \n",
    "1. `autos.cylinders.describe()` - the count is of not null values, so it can get you how many nulls\n",
    "1. `autos.cyclinders.value_counts(dropna = False)` how many nulls\n",
    "1. `autos['drive']` is a low cardinality categorial originally coded as an `object`. `autos.drive.value_counts(dropna = False)` will tell you what the cardinality is. You can assign things to be `category` to save memory `astype('make': 'category')`\n",
    "\n",
    "Summary up to 31:20\n",
    "1. Dot chaining is good. Use `query`, `assign`, `astype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dot chaining example\n",
    "(df\n",
    " [cols]\n",
    "  .select_dtypes(int)\n",
    "  .describe()\n",
    ")\n",
    "\n",
    "#assign types\n",
    "(df\n",
    " [cols]\n",
    "  .astype({'highway08': 'int8', 'city08' : 'int16'})\n",
    "  .describe()\n",
    ")\n",
    "\n",
    "#quickly see missing values\n",
    "(df \n",
    " [cols]\n",
    "  .query('cylinders.isna()')\n",
    ")\n",
    "\n",
    "#impute na's by filling them with zeros\n",
    "(df \n",
    " [cols]\n",
    "  .assign(cylinders = df.cylinders.fillna(0).astype('int8'))\n",
    ")\n",
    "\n",
    "#number of unique values for column drive by year\n",
    "(\n",
    "    df[cols]\n",
    "    .groupby('year')\n",
    "    .drive\n",
    "    .nunique\n",
    ")\n",
    "\n",
    "#split speeds from tran variable\n",
    "(\n",
    "    df[cols]\n",
    "    .assign(automatic = autos.tran.str.contains('Auto'),\n",
    "            speeds = autos.tran.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary pivot table in pandas:\n",
    "1. `df.assign` is a great way to rename columns, make new columns. Can use lambda functions to perform custom transformations on the current version of the dot chain.\n",
    "1. `pivot_table`, will pivot data from long to wide. You will need to set an index, and then either `reset_index()` or `reset_index(0)` to get the data back into a decent shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_usage = (df\n",
    "     .groupby(['perceived_race'])['channel']\n",
    "     .value_counts()\n",
    "     .to_frame()\n",
    "     .rename(columns = {'channel': 'n'})\n",
    "     .reset_index()\n",
    "     .assign(n_total = lambda x: x.groupby('perceived_race')['n'].transform('sum'))\n",
    "     .assign(pct = lambda x: round(100 * x.n / x.n_total, 2))\n",
    "     .pivot_table(index = ['perceived_race', 'n_total'], columns = ['channel'], values = ['n', 'pct'])\n",
    "     .reset_index()\n",
    "     .assign(quarter = quarter)\n",
    ")\n",
    "\n",
    "channel_usage.columns = ['perceived_race', 'n_total', 'n_ib', 'n_rtb', 'pct_ib', 'pct_rtb', 'quarter']\n",
    "\n",
    "channel_usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9509740531b6453a82b6edb50a42a78b997947dc8acb16a59cc097507c8cd385"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
