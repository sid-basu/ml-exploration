{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on SKlearn and Pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied over from `december_logistic_regression_people_analytics.ipynb` on Jan 16 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddba8697-bfa3-4955-9f0c-5d5dd107739e",
   "metadata": {},
   "source": [
    "# SKLearn notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0945f4f2-618a-4321-8efb-402e8b8db141",
   "metadata": {},
   "source": [
    "The  sklearn preprocessing methods have fit and transform methods. `fit` fits the data, while `transform` returns you back data that's centered or one hot encoded. `fit_transform` can do both.\n",
    "\n",
    "The general sklearn pipeline is as follows:\n",
    "1. The `sklearn.model_selection` module will help you split data / randomly sample your data.\n",
    "    1. Use `train_test_split` to split data into train and test\n",
    "    1. The `StratifiedKFold` class is a \"splitter class / cross validator\" that you can use to do cross validation. \n",
    "    1. The `GridSearchCV` is a hyper-paramater optimizer, that will do exhaustive search over specified parameter values for an estimator. You can also call `cross_validate` if you don't need to do full hyperparameter optimization, and you just want average model performance across your K-fold cross validator.\n",
    "1. The `sklearn.preprocessing` module will help you scale / transform columns so that theyre ready for a model. \n",
    "    1. Use a `ColumnTransformer` to do `StandardScaler` or `OneHotEncoder` / `LabelEncoder`. Use the `fit` and `transform` of these preprocessors to make life easier. `fit` fits the scaler/encoder to your data, and `transform` transforms your data with the appropriate rule. `fit_transform` does both at once. You may also need an imputer, such as a `SimpleImputer`.\n",
    "    1. A `ColumnTransformer` will only work with named columns in a Pandas DataFrame, which means you can only use it at the first step of a pipeline (otherwise upstream steps make things into numpy arrays which arent named -- where you can use the numeric indexer if really needed). A `sklearn.compose.make_column_selector` can help with unnamed columns as well. Creating `passthrough` columnn selectors are pretty useful as well.\n",
    "    1. A `FunctionTransformer` can help you construct a transformer from an arbitrary callable. This won't store data between train and test set (eg. if you wanted to impute data with medians).\n",
    "1. Use the `Pipeline` method to make a pipeline with your feature engineering and classifier, `LogisticRegression` in this case.\n",
    "    1. A `sklearn.pipeline.FeatureUnion` can concatenate results of multiple transformer objects. Eg. you do PCA and SVD on the same input dataset. This can be useful for performing different operations for different types of columns. This [blog](https://adamnovotny.com/blog/custom-scikit-learn-pipeline.html) has some good examples.\n",
    "1. Call the `fit` method of your pipeline to fit the data. At this point you can do more advanced model selection:\n",
    "    1. You can do a `GridSearchCV` over a `StratifiedKFold` to do hyperparameter optimization\n",
    "    1. You can also `model_selection.cross_validate` over a `StratifiedKFold` to get the average model performance, without doing hyperparameter optimization. In case you want to do anything more advanced using the `StratifiedKFold`, you will likely need to get the train and test indices from the skf. You can do this by looping through `for train_index, test_index in skf.split(X, y)`.\n",
    "1. The `sklearn.metrics` module will get you precision recall and ROC curves. \n",
    "    1. `metrics.precision_recall_curve` and `metrics.roc_curve` will take predicted probabilities and true outcomes to generate PR and ROC curves with many thresholds. \n",
    "    1. `metrics.accuracy_score` and `metrics.f1_score` will take predicted and true outcomes and give you an accuracy or f1 score. Be careful about whether the function takes `y_pred` (0 or 1) or predicted probabilities as an input.\n",
    "1. To get the predictions of a classifier, you need to use the `predict_proba` and `decision_function` methods (depending on the classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1615ac6",
   "metadata": {},
   "source": [
    "SKlearn tutorial video notes [link](https://www.youtube.com/watch?v=0Lt9w-BxKFQ&t=217s). \n",
    "\n",
    "1. Wine quality dataset, predict quality based on things like acidity, density, alcohol content, pH etc.\n",
    "1. How to process nulls. `wine.isnull().sum`\n",
    "1. Make a LabelEncoder fit/transform on labels.\n",
    "1. Define X and y. You can drop a single column as follows: `wine.drop('quality', axis = 1)`\n",
    "1. `train_test_split`\n",
    "1. Scale input columns with a `StandardScaler`, `X_train = sc.fit_transform(X_train)` (maybe using a pipeline is better).\n",
    "1. Random forest initiate: `rfc = RandomForestClassifier(n_estimators=200)`. Works well in medium sized data\n",
    "1. Fit the random forest: `rfc.fit(X_train, y_train)`\n",
    "1. Predict: `pred_rfc = rfc.predict(X_test)`, might need to use `predict_proba` sometimes\n",
    "1. See how well the model performed: `print(classification_report(y_test, pred_rfc))`\n",
    "1. SVM initiate `clf = svm.SVC()`. Then fit / predict based on them. SVM does better with small datasets generally.\n",
    "1. Neural network (does well with huge data, images, text): `mlpc = MLPClassifier(hidden_layer_sizes = (11, 11, 11), max_iter = 500)`\n",
    "1. Accuracy score: `cm = metrics.accuracy_score(y_test, pred_rfc)` \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to ask you some questions about topics in applied machine learning that doen't really come up in textbooks. I'd love to get your thoughts on these, as well as any other important topics you see in practice that don't come up in most classes.\n",
    "\n",
    "How to impute `NA` values: \n",
    "1. Right now I see a couple possibilities here. One approach is to use `sklearn`'s `SimpleImputer` or `IterativeImputer` to do the work for you. The other is to some exploratory data analysis to look at major patterns in the data (eg. Titanic missing age and survival are very correlated with passenger class and gender, so imputing age with class/gender medians might make sense). I'd love to know what strategies you use to impute missing data. \n",
    "1. I also have noticed that how you impute `NA`'s has a big impact on model performance. How do you keep track of what the best imputation strategy is? Do you treat it as a hyperparameter to be optimized with other variables (eg. tree depth), or do you just deal with it at the training stage (eg. do a bit of k-fold cross validation to understand the best imputation strategy, then fix the imputation strategy and move on?)\n",
    "\n",
    "Other preprocessing:\n",
    "1. Preprocessing seems very important in applied machine learning. Eg, how you encode categoricals (one hot vs ordinal) and numerics (K bins, or standard scaled) can affect your model performance. I was wondering how you would choose the best preprocessing method. Do you deal with it at the training stage (eg. do k-fold cross validation on training data to find the strategy with the best performance, and then fix the imputations strategy moving forward). Or do you do something else?\n",
    "\n",
    "Model validation / k-fold cross validation:\n",
    "1. How to choose the value of k. Eg the Titanic dataset has ~800 observations. My current workflow when testing whether doing something (eg. bucketing features) helps the model or not is doing 10-fold cross validation over a `StratifiedKFold` in sklearn and plot the ROC curves for each test fold. I'll average the ROC curves to get an AUC metric that I use as the overall judge of model performance. Does this seem reasonable? Would you add other metrics? Eg. I also look at AUPRC and test set accuracy in the test folds.\n",
    "\n",
    "How to deal with imbalanced classes:\n",
    "1. A lot of fraud datasets have super imbalanced classes. I was wondering what some strategies you use to deal with this are. Oversampling positives makes a lot of sense. What are some other best practices? Do you still use metrics like AUROC, or do you use a different strategy.\n",
    "\n",
    "Minimizing leakage between train and test sets\n",
    "1. One thing that a lot of courses emphasize is that you don't want to \"leak\" data from your test set to the train set. For example, if you scale your features using the mean and standard deviation in the train and test set, you might leak some information. I have two questions about this:\n",
    "    1. How diligent are you about this? Eg. an imputation strategy for age in the Titanic challenge is to take the median age within each gender / passenger class. This is a major pain to code if you are being diligent about using only training set data for imputation in the training set (eg if you are using K-fold cross validation). I defined custom functions in order to do this imputation, and fed them into sklearn pipelines (which are good for keeping train/test separate). Is this what you would do in this situation? Is this overkill?\n",
    "    1. In a production environment, I assume that you wouldn't be relying on sklearn to do your preprocessing. I was wondering what the overall strategy of doing something like this would be in a production environment. \n",
    "\n",
    "How to keep track of all the different hyperparameters that are to be optimized. It just seems like there are so many things to be tuned, and I get overwhelmed in thinking about how to keep track of them all.\n",
    "1. Preprocessing (imputation, encoding / scaling etc)\n",
    "1. What features to include\n",
    "1. Which model to use (Logistic Regression, XGBoost, Neural Network etc)\n",
    "1. The hyperparameters of the model used. Eg. tree depth, regularization.\n",
    "Do you have any strategies to keep track of everything / optimize each of these categories? Do you fix some things (eg. preprocessing, features) and optimize others later? Any advice on this would be much appreciated.\n",
    "\n",
    "I guess the biggest question is, how do you choose the best model? Do you optimize on accuracy / AUC of a test set? Or something else?\n",
    "\n",
    "\n",
    "Other thoughts (don't ask as questions to Vikram):\n",
    "1. Prediction is difference from inference. In inference you really care about the parameters / model structure. That's less important in prediction (IMO). Prediction really cares about getting a decent prediction for a user (might need to impute etc).\n",
    "1. Even though the mindset of prediction and inference are different, having a base skillset of being curious about data, and being proficient at EDA / general analysis will be very helpful for both inference and algorithms jobs.\n",
    "\n",
    "My own journey (from ~2018 onwards, 2015-17 I was focused on getting better at inference):\n",
    "1. Start as causal inference DS\n",
    "1. 2017-present: Build overall Python skills (messaging analysis, templated queries)\n",
    "1. 2018-2019: CS231n\n",
    "1. H1 2019: Teach DataU 341\n",
    "1. 2020-2021: (fail) Convex Optimization Class\n",
    "1. 2022: Build Python data analysis skills (do analyses in Python, useful crutches are plotnine, duckdb and pandas tutorials)\n",
    "1. 2022 H2: Titanic kaggle\n",
    "\n",
    "Key areas:\n",
    "1. Overall Python\n",
    "1. Data manipulation / plotting in Python\n",
    "1. ML math, basics\n",
    "1. Applied ML\n",
    "1. Production ML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AUC\n",
    "1. Precision / Recall\n",
    "1. F1 Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `pd.merge` to merge datasets\n",
    "1. `df.loc` vs `df.iloc`: integer indexing\n",
    "1. `value_counts` also `data.groupby(groups).size().reset_index().rename(columns = {0:'size'})`\n",
    "1. `df.isna().sum()`\n",
    "1. `df['column'].astype('int')`\n",
    "1. `A.query('Age == median')` to get rows where Age is equal to median\n",
    "1. Dummy column `df.assign(is_ib_test = (df.channel == 'instant_book').astype('int'))`\n",
    "1. Make new column as a function of the value in another column: `df_n_reviews_race.loc[df_n_reviews_race.n_reviews == '5', 'n_reviews'] = '5+'`\n",
    "1. `data.loc[married_mask, 'Married'] = 1` is more stable than `data['Married'].loc[married_mask] = 1` since the latter has \"chaining\" of indices, which can lead to unstable results.\n",
    "1. `data['Deck'] = data['Deck'].replace(['F', 'G'], 'FG')`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46f85706",
   "metadata": {},
   "source": [
    "Pandas tips from sklearn video\n",
    "\n",
    "1. `data.isnull.sum()`\n",
    "1. `pd.cut(wine['quality'], bins = bins, labels = group_names)`\n",
    "1. `wine['quality'].value_counts()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78445a2c",
   "metadata": {},
   "source": [
    "Matt Harrison Effective Pandas video notes [link](https://www.youtube.com/watch?v=UURvPeczxJI&t=505s).\n",
    "1. Pandas is built on top of NumPy, a package to make numerical computation faster in Python.\n",
    "1. `autos[cols].dtypes` you see a lot of `int64` / `float64` (fast) or `object` (mixed type or string).\n",
    "1. `autos[cols].memory_usage(deep = True)` \n",
    "1. You can save memory by converting 16 byte integers to 8 byte integers etc. \n",
    "1. `autos.cylinders.describe()` - the count is of not null values, so it can get you how many nulls\n",
    "1. `autos.cyclinders.value_counts(dropna = False)` how many nulls\n",
    "1. `autos['drive']` is a low cardinality categorial originally coded as an `object`. `autos.drive.value_counts(dropna = False)` will tell you what the cardinality is. You can assign things to be `category` to save memory `astype('make': 'category')`\n",
    "1. `pd.to_datetime` to turn a string to datetime, `replace` to replace strings, `str.contains`\n",
    "1. Have one cell that loads the data, then have one big chain to clean up the data (as a function). Non-chained code in multiple cells is bad. If you don't chain, you use more memory, and code is harder to reproduce.\n",
    "\n",
    "Summary up to 31:20\n",
    "1. Dot chaining is good. Use `query`, `assign`, `astype`. Have one big chain to clean up data up top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dot chaining example\n",
    "(df\n",
    " [cols]\n",
    "  .select_dtypes(int)\n",
    "  .describe()\n",
    ")\n",
    "\n",
    "#assign types\n",
    "(df\n",
    " [cols]\n",
    "  .astype({'highway08': 'int8', 'city08' : 'int16'})\n",
    "  .describe()\n",
    ")\n",
    "\n",
    "#quickly see missing values\n",
    "(df \n",
    " [cols]\n",
    "  .query('cylinders.isna()')\n",
    ")\n",
    "\n",
    "#impute na's by filling them with zeros\n",
    "(df \n",
    " [cols]\n",
    "  .assign(cylinders = df.cylinders.fillna(0).astype('int8'))\n",
    ")\n",
    "\n",
    "#number of unique values for column drive by year\n",
    "(\n",
    "    df[cols]\n",
    "    .groupby('year')\n",
    "    .drive\n",
    "    .nunique\n",
    ")\n",
    "\n",
    "#split speeds from tran variable\n",
    "(\n",
    "    df[cols]\n",
    "    .assign(automatic = autos.tran.str.contains('Auto'),\n",
    "            speeds = autos.tran.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary pivot table in pandas:\n",
    "1. `df.assign` is a great way to rename columns, make new columns. Can use lambda functions to perform custom transformations on the current version of the dot chain.\n",
    "1. `pivot_table`, will pivot data from long to wide. You will need to set an index, and then either `reset_index()` or `reset_index(0)` to get the data back into a decent shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_usage = (df\n",
    "     .groupby(['perceived_race'])['channel']\n",
    "     .value_counts()\n",
    "     .to_frame()\n",
    "     .rename(columns = {'channel': 'n'})\n",
    "     .reset_index()\n",
    "     .assign(n_total = lambda x: x.groupby('perceived_race')['n'].transform('sum'))\n",
    "     .assign(pct = lambda x: round(100 * x.n / x.n_total, 2))\n",
    "     .pivot_table(index = ['perceived_race', 'n_total'], columns = ['channel'], values = ['n', 'pct'])\n",
    "     .reset_index()\n",
    "     .assign(quarter = quarter)\n",
    ")\n",
    "\n",
    "channel_usage.columns = ['perceived_race', 'n_total', 'n_ib', 'n_rtb', 'pct_ib', 'pct_rtb', 'quarter']\n",
    "\n",
    "channel_usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "278fca6d6bfda02810a468218487fe4899bd1051aaa9e02278d60d912b2c34c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
