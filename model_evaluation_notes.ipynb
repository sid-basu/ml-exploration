{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General workflow for ML notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import dataset, do basic data cleaning / feature generation\n",
    "1. Train / test split\n",
    "    1. Often use a time distribution for this (eg. Jan - May train, June test)\n",
    "1. Do a bunch of EDA on data\n",
    "    1. Distributions of each column\n",
    "    1. Looking at how to group categoricals\n",
    "    1. Look at how to impute nulls\n",
    "    1. Relationships of each column with outcome\n",
    "    1. Correlations of columns\n",
    "1. Baseline model (simple logistic regression)\n",
    "1. Fit models on training set\n",
    "    1. Use an sklearn pipeline to do main preprocessing / model fitting\n",
    "    1. Use sklearn classification report to understand precision / recall / f1 score\n",
    "1. Do hyperparameter optimization\n",
    "    1. Can use BayesSearchCV over a stratified k-fold (or a gridsearch?)\n",
    "1. Take the best hyperparameters, train a model on the entire train set\n",
    "1. Analyze test set performance\n",
    "1. Feature importance plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**: If you classify as a positive, what is the probability it is an actual positive?\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{Relevant retrieved instances}}{\\text{All retreived instances}} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "1. Can be of positive class, or of negative class\n",
    "1. Can be dollar weighted. Eg. of the total dollar transaction amount you classify as fraud, how much is actually fraud?\n",
    "\n",
    "**Recall**: Of all the positive instances, how many did you capture?\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{Relevant retrieved instances}}{\\text{All relevant instances}} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "1. Also called \"True positive rate\"\n",
    "1. Can be of positive class, or of negative class\n",
    "1. Can be dollar weighted. Eg. of the total fraudulent transaction volume, how much do you classify as fraud?\n",
    "\n",
    "Precision and recall are threshold-specific metrics. Also look at precision-recall curve and area under precision recall curve to get a full picture of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall at exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-recall curve is sensitive to underlying fraud prevalence.\n",
    "1. When fraud prevalence is low, precision at a specific recall will be lower than if fraud prevalence is higher.\n",
    "1. Can plot recall (y-axis) vs exposure (x-axis):\n",
    "    1. Exposure: percent of good users that we friction (false positives / false positives + true negatives)\n",
    "    1. To compute, look at the score threshold in good user distribution. Eg 5% exposure means look at the 95 percentile score in good users. Then get the recall. What fraction of bad users had a score above the threshold you computed.\n",
    "1. This works because recall is solely calculated on positive examples, and the threshold for an exposure rate is calculated solely based on negative examples. So the ratio of the two classes does not matter.\n",
    "\n",
    "![images](./images/recall_at_exposure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision at k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful for ranking models. Remember to adjust for position bias in ranking model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is the harmonic mean of precision and recall. Therefore it symetrically represents both precision and recall in the computation. \n",
    "1. The point that maximizes F1 score should have high precision and high recall.\n",
    "1. Ranges between 0 and 1. The closer F1 score is to 1, the more accurate your model is.\n",
    "1. Precision and recall explicitly depend on the ratio of positive to negative test cases. That makes F1 score also sensitive to this ratio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} \n",
    "F 1 \\text{ Score} &= \\frac{2}{\\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}}} \\\\\n",
    "\\\\\n",
    "&= 2 \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is robust to imbalanced classes? [link](https://www.picsellia.com/post/understanding-the-f1-score-in-machine-learning-the-harmonic-mean-of-precision-and-recall)\n",
    "\n",
    "You can compute F1 scores for each class (positive and negative, or k classes in multi-class decision making), and then weight the F1 score by number of samples in each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\text{F}\\beta$ score is a weighted version of the F1 score.\n",
    "1. If $\\beta$ is bigger than one, then recall will be overweighted.\n",
    "1. If $\\beta$ is less than one, then precision will be overweighted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} \n",
    "F \\beta \\text{ Score} &= \\frac{1 + \\beta^2}{\\frac{1}{\\text{Precision}} + \\frac{\\beta^2}{\\text{Recall}}} \\\\\n",
    "\\\\\n",
    "&= \\frac{(1 + \\beta^2) \\times \\text{Precision} \\times \\text{Recall}}{(\\beta^2 \\times \\text{Precision} + \\text{Recall})}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under the receiver operator characteristic curve. [link](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc), [good overview of multiple metrics](https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc).\n",
    "\n",
    "**False positive rate** (x-axis). Intuition: \"of all the negatives, how many are false positives\"\n",
    "$$FPR = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "**True positive rate** (recall, y axis). Intuition: \"of all the positives, how many are true positives\"\n",
    "$$TPR = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Some more details:\n",
    "1. Plot the FPR and TPR for multiple thresholds and take the area under the curve to compute the metric.\n",
    "1. If you rank all observations by their model scores, AUROC tells you what the probability of ranking a random positive example higher than a random negative example is.\n",
    "1. AUC is classification threshold invariant, it measures the quality of predictions, irrespective of the threshold.\n",
    "1. AUC is scale invariant. It measures how well predictions are ranked, rather than their absolute values.\n",
    "1. It doesn't do well with heavily imbalanced data. The intuition: false positive rate for highly imbalanced datasets is pulled down due to a large number of true negatives.\n",
    "1. You generally want the curve to be convex, heavily bending to the top left corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other points / questions:\n",
    "1. All of these metrics can be used for threshold selection\n",
    "1. What does a stratified k-fold do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
