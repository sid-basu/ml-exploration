{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General workflow for ML notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import dataset, do basic data cleaning / feature generation\n",
    "1. Train / test split\n",
    "    1. Often use a time distribution for this (eg. Jan - May train, June test)\n",
    "1. Do a bunch of EDA on data\n",
    "    1. Distributions of each column\n",
    "    1. Looking at how to group categoricals\n",
    "    1. Look at how to impute nulls\n",
    "    1. Relationships of each column with outcome\n",
    "    1. Correlations of columns\n",
    "1. Baseline model (simple logistic regression)\n",
    "1. Fit models on training set\n",
    "    1. Use an sklearn pipeline to do main preprocessing / model fitting\n",
    "    1. Use sklearn classification report to understand precision / recall / f1 score\n",
    "1. Do hyperparameter optimization\n",
    "    1. Can use BayesSearchCV over a stratified k-fold (or a gridsearch?)\n",
    "1. Take the best hyperparameters, train a model on the entire train set\n",
    "1. Analyze test set performance\n",
    "1. Set a threshold\n",
    "1. Feature importance plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**: If you classify as a positive, what is the probability it is an actual positive?\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{Relevant retrieved instances}}{\\text{All retreived instances}} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "1. Can be of positive class, or of negative class.\n",
    "1. Can be called positive predictive value or negative predictive value (depending on class).\n",
    "1. Can be dollar weighted. Eg. of the total dollar transaction amount you classify as fraud, how much is actually fraud?\n",
    "\n",
    "**Recall**: Of all the positive instances, how many did you capture?\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{Relevant retrieved instances}}{\\text{All relevant instances}} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "1. Also called \"True positive rate\"\n",
    "1. Can be of positive class, or of negative class\n",
    "1. Can be dollar weighted. Eg. of the total fraudulent transaction volume, how much do you classify as fraud?\n",
    "\n",
    "Precision and recall are threshold-specific metrics. Also look at precision-recall curve and **area under precision recall curve** to get a full picture of the model.\n",
    "1. Area under the precision recall curve can be thought of as the **average of precision scores** calculated for each recall threshold.\n",
    "1. PRAUC is good for imbalanced data (It focuses mainly on the positive class, and cares less about the frequent negative class [Saito and Rehmsmeier](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/). Having lots of true negatives can screw up AUROC and accuracy, but is not reflected in PRAUC).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall at exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-recall curve is sensitive to underlying fraud prevalence.\n",
    "1. When fraud prevalence is low, precision at a specific recall will be lower than if fraud prevalence is higher.\n",
    "1. Can plot recall (y-axis) vs exposure (x-axis):\n",
    "    1. Exposure: percent of good users that we friction (false positives / false positives + true negatives)\n",
    "    1. To compute, look at the score threshold in good user distribution. Eg 5% exposure means look at the 95 percentile score in good users. Then get the recall. What fraction of bad users had a score above the threshold you computed.\n",
    "1. This works because recall is solely calculated on positive examples, and the threshold for an exposure rate is calculated solely based on negative examples. So the ratio of the two classes does not matter.\n",
    "    1. Isn't this just the AUC graph? Need to investigate further.\n",
    "\n",
    "![images](./images/recall_at_exposure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision at k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful for ranking models. Remember to adjust for position bias in ranking model evaluation. [StackOverflow](https://stackoverflow.com/questions/55748792/understanding-precisionk-apk-mapk)\n",
    "1. **P@K**: For example, to calculate P@3: take the top 3 recommendations for a given user and check how many of them are good ones. That number divided by 3 gives you the P@3.\n",
    "1. **AP@K**: For example, to calculate AP@3: sum P@1, P@2 and P@3 and divide that value by 3. AP@K is typically calculated for one user.\n",
    "1. **MAP@K**: For example, to calculate MAP@3: sum AP@3 for all the users and divide that value by the amount of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is the harmonic mean of precision and recall. Therefore it symetrically represents both precision and recall in the computation. \n",
    "1. The point that maximizes F1 score should have high precision and high recall.\n",
    "1. Ranges between 0 and 1. The closer F1 score is to 1, the more accurate your model is.\n",
    "1. Precision and recall explicitly depend on the ratio of positive to negative test cases. That makes F1 score also sensitive to this ratio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} \n",
    "F 1 \\text{ Score} &= \\frac{2}{\\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}}} \\\\\n",
    "\\\\\n",
    "&= 2 \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is robust to imbalanced classes? [link](https://www.picsellia.com/post/understanding-the-f1-score-in-machine-learning-the-harmonic-mean-of-precision-and-recall)\n",
    "\n",
    "You can compute F1 scores for each class (positive and negative, or k classes in multi-class decision making), and then weight the F1 score by number of samples in each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\text{F}\\beta$ score is a weighted version of the F1 score.\n",
    "1. If $\\beta$ is bigger than one, then recall will be overweighted.\n",
    "1. If $\\beta$ is less than one, then precision will be overweighted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} \n",
    "F \\beta \\text{ Score} &= \\frac{1 + \\beta^2}{\\frac{1}{\\text{Precision}} + \\frac{\\beta^2}{\\text{Recall}}} \\\\\n",
    "\\\\\n",
    "&= \\frac{(1 + \\beta^2) \\times \\text{Precision} \\times \\text{Recall}}{(\\beta^2 \\times \\text{Precision} + \\text{Recall})}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under the receiver operator characteristic curve. [link](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc), [Neptune AI overview](https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc).\n",
    "\n",
    "**False positive rate** (x-axis). Intuition: \"of all the negatives, how many are false positives\"\n",
    "$$FPR = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "**True positive rate** (recall, y axis). Intuition: \"of all the positives, how many are true positives\"\n",
    "$$TPR = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Some more details:\n",
    "1. Plot the FPR and TPR for multiple thresholds and take the area under the curve to compute the metric.\n",
    "1. If you rank all observations by their model scores, AUROC tells you what the probability of ranking a random positive example higher than a random negative example is.\n",
    "1. AUC is classification threshold invariant, it measures the quality of predictions, irrespective of the threshold.\n",
    "1. AUC is scale invariant. It measures how well predictions are ranked, rather than their absolute values.\n",
    "1. It doesn't do well with heavily imbalanced data. The intuition: false positive rate for highly imbalanced datasets is pulled down due to a large number of true negatives.\n",
    "1. You generally want the curve to be convex, heavily bending to the top left corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other points / questions:\n",
    "1. All of these metrics can be used for threshold selection\n",
    "1. Neptune AI [Cheatsheet](https://github.com/neptune-ai/blog-binary-classification-metrics/blob/master/binary_classification_metrics_cheathsheet.pdf)\n",
    "    1. Can use a cumulative gain chart to look at how much you would gain using your model over random selection.\n",
    "1. For the home listing recommendation challenge, can use MAP@K as evaluation metric, weighting click / host contact / book, 1, 2, 3 respectively. Can also look at precision / recall (they reflect how many clicks/contacts/bookings we can measure) as well as AUC, which tells us more about the probability and probability ranking. \n",
    "1. What does a stratified k-fold do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do cross validation / hyperparameter optimization job well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AUC generally ranges between .84 and .88 depending on how the training / test data are split\n",
    "    1. The bayesian optimization can lead to slightly different optimal model parameters as well\n",
    "    1. Un-optimized test set AUC of .84, increases to AUC of .86 after HPO. Train set AUC of .87.\n",
    "1. Don't forget to set a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
